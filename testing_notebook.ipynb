{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    # pdf2image might not be installed in all environments\n",
    "    from pdf2image import convert_from_bytes\n",
    "    HAVE_PDF2IMAGE = True\n",
    "except ImportError:\n",
    "    print(\"pdf2image not installed. PDF OCR will fail.\")\n",
    "    HAVE_PDF2IMAGE = False\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_data(eml_file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a .eml file, return a dictionary containing:\n",
    "      {\n",
    "        \"from\": str or None,\n",
    "        \"to\": str or None,\n",
    "        \"subject\": str or None,\n",
    "        \"body\": str or None,      # Prefer plain text if available\n",
    "        \"attachments\": list of {\n",
    "            \"filename\": str,\n",
    "            \"content_type\": str,\n",
    "            \"data\": bytes\n",
    "        }\n",
    "      }\n",
    "\n",
    "    Args:\n",
    "        eml_file_path (str): Full path to the .eml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the extracted data.\n",
    "    \"\"\"\n",
    "    # 1) Read the raw .eml text\n",
    "    with open(eml_file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw_email = f.read()\n",
    "\n",
    "    # 2) Parse into an EmailMessage (new-style) using a modern policy\n",
    "    msg = email.message_from_string(raw_email, policy=email.policy.default)\n",
    "\n",
    "    # 3) Extract top-level headers\n",
    "    from_ = msg.get(\"From\", \"\")\n",
    "    to_ = msg.get(\"To\", \"\")\n",
    "    subject = msg.get(\"Subject\", \"\")\n",
    "\n",
    "    # 4) Walk the parts to find the best body and attachments\n",
    "    body_text = None\n",
    "    attachments = []\n",
    "\n",
    "    for part in msg.walk():\n",
    "        # If it's a container (multipart/*), skip it—we want actual payload parts\n",
    "        if part.get_content_maintype() == \"multipart\":\n",
    "            continue\n",
    "\n",
    "        # Check for attachments\n",
    "        filename = part.get_filename()\n",
    "        if filename:\n",
    "            # It's an attachment\n",
    "            attach_data = part.get_payload(decode=True)\n",
    "            attachments.append({\n",
    "                \"filename\": filename,\n",
    "                \"content_type\": part.get_content_type(),\n",
    "                \"data\": attach_data\n",
    "            })\n",
    "        else:\n",
    "            # Potentially a text part (plain or html or something else)\n",
    "            ctype = part.get_content_type()\n",
    "            if ctype == \"text/plain\":\n",
    "                # If we haven't chosen a body yet, or we prefer plain text,\n",
    "                # decode it here\n",
    "                if body_text is None:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    charset = part.get_content_charset() or \"utf-8\"\n",
    "                    body_text = payload.decode(charset, errors=\"replace\")\n",
    "\n",
    "            elif ctype == \"text/html\":\n",
    "                # Optionally handle HTML if no plain text was found\n",
    "                if body_text is None:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    charset = part.get_content_charset() or \"utf-8\"\n",
    "                    body_text = payload.decode(charset, errors=\"replace\")\n",
    "\n",
    "    # Build the result\n",
    "    result = {\n",
    "        \"from\": from_,\n",
    "        \"to\": to_,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body_text,\n",
    "        \"attachments\": attachments\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_email = extract_email_data(\"./emails_attachments/email_pdf_text_1.eml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame([example_email])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Consider yourself as a customer bank request classification expert who can classify any kind of customer bank request emails. You need to classify them into below request and subrequest categories {\n",
    "  \"Adjustment\": {\n",
    "    \"General Adjustment\": \"For modifications in payment terms or amounts.\",\n",
    "    \"subrequest\": {\n",
    "      \"Rate Correction Adjustment\": \"Adjustments targeting specific rate errors.\",\n",
    "      \"Data Correction\": \"Fixes for data entry mistakes.\"\n",
    "    }\n",
    "  },\n",
    "  \"AU Transfer\": {\n",
    "    \"Standard AU Transfer\": \"Routine authorized transfers.\",\n",
    "    \"subrequest\": {\n",
    "      \"Intra-AU Reallocation\": \"Reassigning funds within segments under the same authorization.\",\n",
    "      \"Cross-Entity Transfer\": \"Transfers between subsidiaries/legal entities within the same group.\",\n",
    "      \"Split Transfer Request\": \"Single transfer split among multiple destination accounts.\"\n",
    "    }\n",
    "  },\n",
    "  \"Closing Notice\": {\n",
    "    \"subrequest\": {\n",
    "      \"Reallocation Fees\": \"\",\n",
    "      \"Reallocation Principal\": \"\",\n",
    "      \"Amendment Fees\": \"\",\n",
    "      \"Partial Settlement Notice\": \"For partial settlements requiring separate tracking.\",\n",
    "      \"Overpayment Notification\": \"To handle overpayments distinctly.\",\n",
    "      \"Early Closure Notification\": \"Facility closed before scheduled date.\",\n",
    "      \"Deferred or Delayed Closure\": \"Postponed closure due to issues.\",\n",
    "      \"Document Submission Requirement\": \"Closure pending due to missing documentation.\"\n",
    "    }\n",
    "  },\n",
    "  \"Commitment Change\": {\n",
    "    \"subrequest\": {\n",
    "      \"Cashless Roll\": \"\",\n",
    "      \"Decrease\": \"\",\n",
    "      \"Increase\": \"\",\n",
    "      \"Drawdown Revision\": \"Adjustments to scheduled drawdown amounts.\",\n",
    "      \"Borrowing Base Recalculation\": \"Triggered by changes in collateral value.\",\n",
    "      \"Commitment Reaffirmation\": \"Reaffirming existing commitment without altering financials.\",\n",
    "      \"Term Extension\": \"Extending the facility term.\",\n",
    "      \"Covenant Update\": \"Changes in covenants like financial ratios.\"\n",
    "    }\n",
    "  },\n",
    "  \"Fee Payment\": {\n",
    "    \"subrequest\": {\n",
    "      \"Ongoing Fee\": \"\",\n",
    "      \"Letter of Credit Fee\": \"\",\n",
    "      \"Retroactive Fee Correction\": \"Adjusting fees from previous periods.\",\n",
    "      \"Fee Allocation Across Accounts\": \"Splitting fees across accounts.\",\n",
    "      \"Fee Waiver Request\": \"Request to waive fees.\",\n",
    "      \"Fee Reversal\": \"Reversing erroneously charged fees.\"\n",
    "    }\n",
    "  },\n",
    "  \"Money Movement Inbound\": {\n",
    "    \"subrequest\": {\n",
    "      \"Principal\": \"\",\n",
    "      \"Interest\": \"\",\n",
    "      \"Principal + Interest\": \"\",\n",
    "      \"Principal + Interest + Fee\": \"\",\n",
    "      \"Consolidated Payment Notification\": \"Aggregating multiple incoming payments.\",\n",
    "      \"Segregated or Partitioned Payment\": \"Earmarking received funds for different purposes.\",\n",
    "      \"Currency Conversion Inbound\": \"Inbound payment received in foreign currency.\",\n",
    "      \"Escrow Payment\": \"Funds directed into escrow.\",\n",
    "      \"Partial Payment\": \"Portion of expected payment received.\"\n",
    "    }\n",
    "  },\n",
    "  \"Money Movement Outbound\": {\n",
    "    \"subrequest\": {\n",
    "      \"Timebound\": \"\",\n",
    "      \"Foreign Currency\": \"\",\n",
    "      \"Failed Transfer Resolution\": \"Corrective action for failed outbound transfers.\",\n",
    "      \"Reversal and Reissue\": \"Reversing and correctly reissuing outbound payments.\",\n",
    "      \"Scheduled Outbound Payment Notification\": \"Future-dated outbound payment setup.\",\n",
    "      \"Automated Reversal\": \"Automated error-triggered reversal process.\",\n",
    "      \"Fee Refund\": \"Returning fees as part of outbound transaction.\"\n",
    "    }\n",
    "  },\n",
    "  \"Additional Process/Operational Exceptions\": {\n",
    "    \"subrequest\": {\n",
    "      \"System-Generated Error Correction\": \"Correcting system-generated errors.\",\n",
    "      \"Manual Intervention Alert\": \"Request for manual review due to unexpected data.\"\n",
    "    }\n",
    "  },\n",
    "  \"Collateral Management\": {\n",
    "    \"subrequest\": {\n",
    "      \"Collateral Revaluation Request\": \"Request for updated or additional collateral.\",\n",
    "      \"Collateral Deficiency Notification\": \"Insufficient collateral notification.\"\n",
    "    }\n",
    "  },\n",
    "  \"Documentation & Compliance\": {\n",
    "    \"subrequest\": {\n",
    "      \"Missing Documentation Follow-up\": \"Follow-up for unreceived documentation.\",\n",
    "      \"Digital Signature Verification Issue\": \"Invalid digital signature flag.\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "Output format:{\"request_type\":, \"subrequest_type\":}. Please don't provide anything other than the json\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractCorrectAnswer(BaseModel):\n",
    "    request_type: str\n",
    "    subrequest_type: str\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_benefits(prompt: str, data_dict: str) -> dict:\n",
    "    ollama_api_url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "    # Ensure the prompt is structured correctly\n",
    "    full_prompt = f\"{prompt}\\n\\n email: {data_dict}\"\n",
    "\n",
    "    # Request payload for Ollama\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": 101,\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "        error = True\n",
    "\n",
    "        while error == True:\n",
    "        \n",
    "            response = requests.post(ollama_api_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Extract response JSON correctly\n",
    "            response_json = response.json()\n",
    "            model_response = response_json.get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "            \n",
    "            # model_response = model_response.split(\"</think>\")[-1][9:-3]\n",
    "            \n",
    "            print(model_response)\n",
    "\n",
    "            # Convert and validate JSON response\n",
    "            parsed_response = json.loads(model_response)  # Extract valid JSON\n",
    "            validated_response = ExtractCorrectAnswer(**parsed_response)\n",
    "            \n",
    "            return validated_response.model_dump()  # Return as dictionary\n",
    "        \n",
    "        \n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError, ValueError) as e:\n",
    "        print(f\"Error parsing model response: {e}\")\n",
    "        return  {\"request_type\": \"manual\", \"subrequest_type\":None}\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        print(f\"Request Error: {re}\")\n",
    "        return {\"request_type\": \"manual\", \"subrequest_type\":None}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected Error: {e}\")\n",
    "        return {\"request_type\": \"manual\", \"subrequest_type\":None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "    return text\n",
    "\n",
    "final_df['subject'] = final_df['subject'].apply(clean_text)\n",
    "final_df['body'] = final_df['body'].apply(clean_text)\n",
    "final_df['attachments'] = final_df['attachments'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force all fields to string type and clean them\n",
    "final_df['subject'] = final_df['subject'].astype(str).str.encode('utf-8', errors='ignore').str.decode('utf-8', errors='ignore')\n",
    "final_df['body'] = final_df['body'].astype(str).str.encode('utf-8', errors='ignore').str.decode('utf-8', errors='ignore')\n",
    "final_df['attachments'] = final_df['attachments'].astype(str).str.encode('utf-8', errors='ignore').str.decode('utf-8', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_json = json.loads(final_df[['subject', 'body', 'attachments']].to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image_for_ocr(pil_img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to an OpenCV (NumPy) image, apply grayscale\n",
    "    and adaptive threshold for better OCR on noisy/uneven scans.\n",
    "    \"\"\"\n",
    "    img = np.array(pil_img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        31, 2\n",
    "    )\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def run_ocr_on_pil(pil_img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Apply OpenCV preprocessing to a PIL Image,\n",
    "    then run Tesseract with custom config.\n",
    "    \"\"\"\n",
    "    processed = preprocess_image_for_ocr(pil_img)\n",
    "    config_str = r\"--oem 3 --psm 6\"\n",
    "    pil_processed = Image.fromarray(processed)\n",
    "    text = pytesseract.image_to_string(pil_processed, config=config_str)\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_pdf(pdf_data: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Convert PDF bytes to images (pdf2image), then\n",
    "    do OCR on each page. Returns full text.\n",
    "    \"\"\"\n",
    "    if not HAVE_PDF2IMAGE:\n",
    "        return \"[Error: pdf2image not installed, cannot OCR PDF]\"\n",
    "    \n",
    "    pages = convert_from_bytes(pdf_data)\n",
    "    all_texts = []\n",
    "    for page_img in pages:\n",
    "        page_text = run_ocr_on_pil(page_img)\n",
    "        all_texts.append(page_text)\n",
    "    return \"\\n\".join(all_texts)\n",
    "\n",
    "\n",
    "def ocr_image(img_data: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Load the raw image (PNG/JPG/etc.) into PIL,\n",
    "    do OpenCV-based preprocessing, and run Tesseract.\n",
    "    \"\"\"\n",
    "    with io.BytesIO(img_data) as buf:\n",
    "        pil_img = Image.open(buf).convert(\"RGB\")\n",
    "        text = run_ocr_on_pil(pil_img)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_jso(args):\n",
    "    \"\"\"\n",
    "    Processes a single jso dictionary.\n",
    "\n",
    "    Args:\n",
    "        args (tuple): Typically (jso, prompt)\n",
    "\n",
    "    Returns:\n",
    "        The output from find_benefits(...) for this jso\n",
    "    \"\"\"\n",
    "    jso, prompt = args\n",
    "\n",
    "    temp_json = eval(jso[\"attachments\"])\n",
    "\n",
    "    if temp_json:\n",
    "        text = \"\"\n",
    "        for index, attachment in enumerate(temp_json):\n",
    "            attachment_text = ocr_pdf(attachment[\"data\"]) \n",
    "            text += f\"attachement_{index}{attachment_text}\"\n",
    "\n",
    "\n",
    "        final_text_sent_model = text + jso[\"subject\"] + jso[\"body\"]\n",
    "\n",
    "    else:\n",
    "        final_text_sent_model = jso[\"subject\"] + jso[\"body\"]\n",
    "\n",
    "\n",
    "    output = find_benefits(prompt, final_text_sent_model)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_process(final_df_json, prompt):\n",
    "    \"\"\"\n",
    "    Runs 'process_jso' in parallel over final_df_json,\n",
    "    preserving order. Returns a list of results in the\n",
    "    same order as final_df_json.\n",
    "    \"\"\"\n",
    "\n",
    "    args_list = [(jso, prompt) for jso in final_df_json]\n",
    "\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "\n",
    "        results = list(tqdm(pool.map(process_jso, args_list),\n",
    "                            total=len(args_list),\n",
    "                            desc=\"Processing JSON items\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"request_type\": \"Adjustment\", \"subrequest_type\": \"General Adjustment\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON items: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = parallel_process(final_df_json, prompt)\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractCorrectAnswer(BaseModel):\n",
    "\n",
    "    account_number: str| None = None\n",
    "    principle_amount:float| None = None\n",
    "    interest:float| None = None\n",
    "    fees:float| None = None\n",
    "    escalation:str| None = None\n",
    "    appreciation:str| None = None\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Consider yourself as a customer bank request classification expert who can classify any kind of customer bank request emails. You need to  identify account_number, principle_amount, interest, fees, escalation, appreciation. If any of these are not present return null. Also, just give the json output and nothing else.\n",
    "\n",
    ".Output should only be a json with all these keys present in json. If any of the keys doesn't have value give null as value { \"account_number\":float, \"principle_amount\":float, \"interest\":float, \"fees\":float , \"escalation\":str ,\"appreciation\": str} Please don't provide anything other than the json\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes(prompt: str, data_dict: str) -> dict:\n",
    "    ollama_api_url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "    # Ensure the prompt is structured correctly\n",
    "    full_prompt = f\"{prompt}\\n\\n email: {data_dict}\"\n",
    "\n",
    "    # Request payload for Ollama\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": 101,\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "\n",
    "        error = True\n",
    "\n",
    "        while error == True:\n",
    "        \n",
    "            response = requests.post(ollama_api_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Extract response JSON correctly\n",
    "            response_json = response.json()\n",
    "            model_response = response_json.get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "            \n",
    "            # model_response = model_response.split(\"</think>\")[-1][9:-3]\n",
    "            \n",
    "            print(model_response)\n",
    "\n",
    "            # Convert and validate JSON response\n",
    "            parsed_response = json.loads(model_response)  # Extract valid JSON\n",
    "            validated_response = ExtractCorrectAnswer(**parsed_response)\n",
    "            \n",
    "            return validated_response.model_dump()  # Return as dictionary\n",
    "        \n",
    "        \n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError, ValueError) as e:\n",
    "        print(f\"Error parsing model response: {e}\")\n",
    "        return  {\"request_type\": \"manual\", \"subrequest_type\":None}\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        print(f\"Request Error: {re}\")\n",
    "        return {\"request_type\": \"manual\", \"subrequest_type\":None}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected Error: {e}\")\n",
    "        return {\"request_type\": \"manual\", \"subrequest_type\":None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_jso(args):\n",
    "    \"\"\"\n",
    "    Processes a single jso dictionary.\n",
    "\n",
    "    Args:\n",
    "        args (tuple): Typically (jso, prompt)\n",
    "\n",
    "    Returns:\n",
    "        The output from find_benefits(...) for this jso\n",
    "    \"\"\"\n",
    "    jso, prompt = args\n",
    "\n",
    "    temp_json = eval(jso[\"attachments\"])\n",
    "\n",
    "    if temp_json:\n",
    "        text = \"\"\n",
    "        for index, attachment in enumerate(temp_json):\n",
    "            attachment_text = ocr_pdf(attachment[\"data\"]) \n",
    "            text += f\"attachement_{index}{attachment_text}\"\n",
    "\n",
    "\n",
    "        final_text_sent_model = text + jso[\"subject\"] + jso[\"body\"]\n",
    "\n",
    "    else:\n",
    "        final_text_sent_model = jso[\"subject\"] + jso[\"body\"]\n",
    "\n",
    "\n",
    "    output = find_attributes(prompt, final_text_sent_model)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parallel_process(final_df_json, prompt):\n",
    "    \"\"\"\n",
    "    Runs 'process_jso' in parallel over final_df_json,\n",
    "    preserving order. Returns a list of results in the\n",
    "    same order as final_df_json.\n",
    "    \"\"\"\n",
    "\n",
    "    args_list = [(jso, prompt) for jso in final_df_json]\n",
    "\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "\n",
    "        results = list(tqdm(pool.map(process_jso, args_list),\n",
    "                            total=len(args_list),\n",
    "                            desc=\"Processing JSON items\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"account_number\": null,\n",
      "  \"principle_amount\": null,\n",
      "  \"interest\": null,\n",
      "  \"fees\": null,\n",
      "  \"escalation\": \"Payment term adjustment\",\n",
      "  \"appreciation\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON items: 100%|██████████| 1/1 [00:00<00:00, 8355.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs_attributes = parallel_process(final_df_json, prompt)\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
